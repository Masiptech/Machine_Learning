{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Network\n",
    "\n",
    "This notebook introduces how to train a neural network using scikit learn. If you are interested to learn more about this python module, you can study from this link:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "We will use the data we had from our last class, and let's try to train a simple multi-layer perceptron. As always, we first load our data using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt('data1.txt')\n",
    "\n",
    "x = data[:,0:2]\n",
    "y = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4186, 2)\n",
      "(1795, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code creates multi-layer perceptron model from a scikit learn module. We call `model.fit(x,y)` to quickly train our network, using only train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70655479\n",
      "Iteration 2, loss = 0.69248262\n",
      "Iteration 3, loss = 0.68000919\n",
      "Iteration 4, loss = 0.66865143\n",
      "Iteration 5, loss = 0.65834875\n",
      "Iteration 6, loss = 0.64827813\n",
      "Iteration 7, loss = 0.63844986\n",
      "Iteration 8, loss = 0.62861719\n",
      "Iteration 9, loss = 0.61860293\n",
      "Iteration 10, loss = 0.60826406\n",
      "Iteration 11, loss = 0.59759755\n",
      "Iteration 12, loss = 0.58648396\n",
      "Iteration 13, loss = 0.57498576\n",
      "Iteration 14, loss = 0.56313701\n",
      "Iteration 15, loss = 0.55097674\n",
      "Iteration 16, loss = 0.53850245\n",
      "Iteration 17, loss = 0.52585719\n",
      "Iteration 18, loss = 0.51307735\n",
      "Iteration 19, loss = 0.50025817\n",
      "Iteration 20, loss = 0.48743496\n",
      "Iteration 21, loss = 0.47473962\n",
      "Iteration 22, loss = 0.46218836\n",
      "Iteration 23, loss = 0.44986334\n",
      "Iteration 24, loss = 0.43784181\n",
      "Iteration 25, loss = 0.42608405\n",
      "Iteration 26, loss = 0.41467835\n",
      "Iteration 27, loss = 0.40366215\n",
      "Iteration 28, loss = 0.39298444\n",
      "Iteration 29, loss = 0.38269485\n",
      "Iteration 30, loss = 0.37283042\n",
      "Iteration 31, loss = 0.36332249\n",
      "Iteration 32, loss = 0.35420782\n",
      "Iteration 33, loss = 0.34548473\n",
      "Iteration 34, loss = 0.33712762\n",
      "Iteration 35, loss = 0.32912429\n",
      "Iteration 36, loss = 0.32147150\n",
      "Iteration 37, loss = 0.31414900\n",
      "Iteration 38, loss = 0.30716474\n",
      "Iteration 39, loss = 0.30048661\n",
      "Iteration 40, loss = 0.29409725\n",
      "Iteration 41, loss = 0.28800700\n",
      "Iteration 42, loss = 0.28217733\n",
      "Iteration 43, loss = 0.27662134\n",
      "Iteration 44, loss = 0.27131730\n",
      "Iteration 45, loss = 0.26621690\n",
      "Iteration 46, loss = 0.26134461\n",
      "Iteration 47, loss = 0.25671567\n",
      "Iteration 48, loss = 0.25226559\n",
      "Iteration 49, loss = 0.24801718\n",
      "Iteration 50, loss = 0.24394874\n",
      "Iteration 51, loss = 0.24007043\n",
      "Iteration 52, loss = 0.23629804\n",
      "Iteration 53, loss = 0.23274961\n",
      "Iteration 54, loss = 0.22930862\n",
      "Iteration 55, loss = 0.22604084\n",
      "Iteration 56, loss = 0.22289281\n",
      "Iteration 57, loss = 0.21985564\n",
      "Iteration 58, loss = 0.21697197\n",
      "Iteration 59, loss = 0.21419446\n",
      "Iteration 60, loss = 0.21152434\n",
      "Iteration 61, loss = 0.20895556\n",
      "Iteration 62, loss = 0.20649733\n",
      "Iteration 63, loss = 0.20412591\n",
      "Iteration 64, loss = 0.20187339\n",
      "Iteration 65, loss = 0.19966552\n",
      "Iteration 66, loss = 0.19758203\n",
      "Iteration 67, loss = 0.19554512\n",
      "Iteration 68, loss = 0.19361762\n",
      "Iteration 69, loss = 0.19173752\n",
      "Iteration 70, loss = 0.18994820\n",
      "Iteration 71, loss = 0.18821130\n",
      "Iteration 72, loss = 0.18653376\n",
      "Iteration 73, loss = 0.18493598\n",
      "Iteration 74, loss = 0.18338193\n",
      "Iteration 75, loss = 0.18188522\n",
      "Iteration 76, loss = 0.18043302\n",
      "Iteration 77, loss = 0.17907434\n",
      "Iteration 78, loss = 0.17771624\n",
      "Iteration 79, loss = 0.17642398\n",
      "Iteration 80, loss = 0.17516014\n",
      "Iteration 81, loss = 0.17399249\n",
      "Iteration 82, loss = 0.17280938\n",
      "Iteration 83, loss = 0.17168232\n",
      "Iteration 84, loss = 0.17061631\n",
      "Iteration 85, loss = 0.16955981\n",
      "Iteration 86, loss = 0.16855990\n",
      "Iteration 87, loss = 0.16757143\n",
      "Iteration 88, loss = 0.16665078\n",
      "Iteration 89, loss = 0.16572378\n",
      "Iteration 90, loss = 0.16485644\n",
      "Iteration 91, loss = 0.16400294\n",
      "Iteration 92, loss = 0.16317896\n",
      "Iteration 93, loss = 0.16240179\n",
      "Iteration 94, loss = 0.16161353\n",
      "Iteration 95, loss = 0.16089608\n",
      "Iteration 96, loss = 0.16016820\n",
      "Iteration 97, loss = 0.15948069\n",
      "Iteration 98, loss = 0.15880287\n",
      "Iteration 99, loss = 0.15815359\n",
      "Iteration 100, loss = 0.15753326\n",
      "Iteration 101, loss = 0.15691450\n",
      "Iteration 102, loss = 0.15635709\n",
      "Iteration 103, loss = 0.15576729\n",
      "Iteration 104, loss = 0.15522181\n",
      "Iteration 105, loss = 0.15469304\n",
      "Iteration 106, loss = 0.15417737\n",
      "Iteration 107, loss = 0.15368668\n",
      "Iteration 108, loss = 0.15319036\n",
      "Iteration 109, loss = 0.15274899\n",
      "Iteration 110, loss = 0.15229847\n",
      "Iteration 111, loss = 0.15184891\n",
      "Iteration 112, loss = 0.15142226\n",
      "Iteration 113, loss = 0.15103706\n",
      "Iteration 114, loss = 0.15062789\n",
      "Iteration 115, loss = 0.15025713\n",
      "Iteration 116, loss = 0.14988240\n",
      "Iteration 117, loss = 0.14952401\n",
      "Iteration 118, loss = 0.14917697\n",
      "Iteration 119, loss = 0.14885149\n",
      "Iteration 120, loss = 0.14853397\n",
      "Iteration 121, loss = 0.14820779\n",
      "Iteration 122, loss = 0.14789804\n",
      "Iteration 123, loss = 0.14762534\n",
      "Iteration 124, loss = 0.14733308\n",
      "Iteration 125, loss = 0.14705610\n",
      "Iteration 126, loss = 0.14679556\n",
      "Iteration 127, loss = 0.14652241\n",
      "Iteration 128, loss = 0.14628190\n",
      "Iteration 129, loss = 0.14603614\n",
      "Iteration 130, loss = 0.14580011\n",
      "Iteration 131, loss = 0.14561049\n",
      "Iteration 132, loss = 0.14535648\n",
      "Iteration 133, loss = 0.14514582\n",
      "Iteration 134, loss = 0.14494880\n",
      "Iteration 135, loss = 0.14475528\n",
      "Iteration 136, loss = 0.14457322\n",
      "Iteration 137, loss = 0.14436492\n",
      "Iteration 138, loss = 0.14418112\n",
      "Iteration 139, loss = 0.14401035\n",
      "Iteration 140, loss = 0.14387140\n",
      "Iteration 141, loss = 0.14368351\n",
      "Iteration 142, loss = 0.14352145\n",
      "Iteration 143, loss = 0.14337329\n",
      "Iteration 144, loss = 0.14321837\n",
      "Iteration 145, loss = 0.14307289\n",
      "Iteration 146, loss = 0.14293047\n",
      "Iteration 147, loss = 0.14281597\n",
      "Iteration 148, loss = 0.14265171\n",
      "Iteration 149, loss = 0.14254697\n",
      "Iteration 150, loss = 0.14241557\n",
      "Iteration 151, loss = 0.14228380\n",
      "Iteration 152, loss = 0.14217223\n",
      "Iteration 153, loss = 0.14205673\n",
      "Iteration 154, loss = 0.14195671\n",
      "Iteration 155, loss = 0.14185206\n",
      "Iteration 156, loss = 0.14174643\n",
      "Iteration 157, loss = 0.14163345\n",
      "Iteration 158, loss = 0.14153564\n",
      "Iteration 159, loss = 0.14144734\n",
      "Iteration 160, loss = 0.14135203\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=5, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=50000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=True, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(activation='logistic', hidden_layer_sizes=(5), max_iter=50000, verbose=True)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been fit, we can use the model to predict class labels from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for train dataset: \n",
      "94.6966077401 %\n",
      "\n",
      "accuracy score for test dataset: \n",
      "94.9860724234 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat_train = model.predict(x_train)\n",
    "acc_train = accuracy_score(y_train, yhat_train)\n",
    "print(\"accuracy score for train dataset: \")\n",
    "print(str(acc_train*100) + \" %\\n\")\n",
    "\n",
    "yhat_test = model.predict(x_test)\n",
    "acc_test = accuracy_score(y_test, yhat_test)\n",
    "print(\"accuracy score for test dataset: \")\n",
    "print(str(acc_test*100) + \" %\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
